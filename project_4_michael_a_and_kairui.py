# -*- coding: utf-8 -*-
"""Project 4 Michael A and Kairui

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lnWvf-gVHJzmsGxR2JskvPupr_Z4faAs

## Comparisions with 2008 Global Financial Crisis, and Investment Recommendations going forward.

You are working in the quantitative finance division of \<Team Name\> Investment Management, LLC, specializing in actively trading financial markets to maximize the returns to investment partners which minimizing downside volatility. Recently, the Chief Investment Officer has become growingly alarmed due to the collaspe of Silicon Valley Bank and Signature Bank, and wonders if this is the repeat of the Global Financial Crisis of 2008. You are tasked to produce a presentation to highlight the similiarities or differences between 2008 and 2023, and also make factor recommendations to steer portfolio management to appropriate exposures.

## What happened during the Global Financial Crisis of 2008?

In 2008, the world experienced a severe global financial crisis that had significant impacts on the global economy. The crisis originated in the United States with the collapse of the housing market and the subprime mortgage industry. As housing prices fell, many borrowers found themselves unable to pay their mortgages, leading to a wave of foreclosures.

This created a ripple effect throughout the financial system, as many banks and financial institutions had invested heavily in these subprime mortgages and related securities. As the value of these assets plummeted, it caused a crisis of confidence in the financial sector, with many banks becoming insolvent and requiring government bailouts.

The crisis quickly spread beyond the US to become a global phenomenon, affecting financial institutions, economies, and individuals around the world. The crisis had a significant impact on the global economy, causing a recession and high levels of unemployment in many countries.

The crisis also led to increased regulation of the financial industry and a renewed focus on risk management and transparency. The events of 2008 continue to have a lasting impact on the global financial system and remain an important area of study for economists, policymakers, and investors alike.

Video: https://drive.google.com/file/d/1uVIa4sgUCtDcktawQ-2XpyDoWwp7pDpy/view?usp=share_link

# Data

## Features for 2007-2008 GFC, features from 2022 for comparision purposes and prices from 2007-2008.

* For 2022-2023 prices, use Yahoo Finance.
"""

!gdown 143ZJibOZUx23y1FV_3Q7c_GS-7t6OGPL
!gdown 1qoBMXSfpo77DQHM6dyKzWSABhAUaRk9g
!gdown 1x5E2vykVpi_McXSeLtj_X_uPDkWWLE9O
import pandas as pd
price_2007=pd.read_parquet('/content/2007_GFC_Prices.par')
features_2007=pd.read_parquet('/content/gfc.parquet')
features_2022=pd.read_parquet('/content/2023.parquet')

"""Let's sort all dataframes chronologically

"""

price_2007=price_2007.sort_values('dint')
features_2007=features_2007.sort_values('qint')
features_2022=features_2022.sort_values('qint')

features_2022

"""## Bloomberg Risk model from 2022

* Even with a Bloomberg terminal, you cannot get the Bloomberg Risk model from 2007-2008

## Possible project task - infer Bloomberg Risk Model for 2008

  * Todo: Feature engineer your own risk model for 2022 to match Bloomberg Risk model from 2022 (This will be the train set)
  * Use that model to create a matching risk model for 2007-2008. (This will be the test set)
"""

import pandas as pd
! pip install gdown
! gdown 1UEPFTkDHL3r5WbKMLWUS9C5ZG7BU3Kk3
! tar -xvf 202303.tar
blgfiles=! ls *.xlsx
# Split each string in the list by whitespace to create a list of filenames
filename_lists = [s.split() for s in blgfiles]

# Flatten the list of lists into a single list of filenames
filenames = [filename for sublist in filename_lists for filename in sublist]

listOfDataFrames=[]
for fn in filenames:
  df=pd.read_excel(fn,engine="openpyxl")
  print(df.columns[3])
  listOfDataFrames.append(df.set_index('Ticker'))

blgdf=pd.concat(listOfDataFrames,axis=1)
blgdf = blgdf.loc[:, ~blgdf.columns.duplicated()]
blgdf=blgdf.drop('Quintile',axis=1)

blgdf.index=blgdf.index.map(lambda s:s.split()[0])

def cleanStr(s):
  mult=1
  s=str(s)
  if s=='--':
    return 0
  if s[-1]=='Ïƒ':
    s=s[:-1]
  if s[-1]=='%':
    s=s[:-1]
    mult=0.01
  return float(s)*mult

for c in blgdf.columns:
  if c in ['Sector', 'Industry']:
    continue
  blgdf[c]=blgdf[c].map(cleanStr)
blgdf.columns=blgdf.columns.map(lambda s:s.split()[-1])

blgnames=['Size', 'Yield', 'Activity',
       'Growth', 'Leverage', 'Momentum', 'Variability', 'Volatility', 'Value',
       'Profit']
blgdf



"""# 1 - Data Cleaning/Exploratory Data Analysis



"""

!pip install gics
from gics import GICS

GICS('40').level(1).name

"""* IBM closed on 3/2007 and 3/2009 at around the same price.
* An investor would not have any capital gains if the price were the same.
* Note that the industry standard is to have prices split adjusted, but not
dividend adjusted.
* Calculate Adj Close and Adj Open by multiply the raw value with dividendFactor.

"""

features_2007.loc[features_2007.ticker=='IBM',['dint','close','dividendFactor']]

features_2007['AdjClose']=features_2007['close']*features_2007['dividendFactor']

features_2007.loc[features_2007.ticker=='IBM',['dint','AdjClose','dividendFactor']]

"""Now we can calculate returns based on AdjClose

* We use securityID as a invariant symbol.
* Stocks can change tickers over time, but securityID will remain constant.
  * FB --> META as an example.
* Forward returns are forward looking returns, so we shift(-1) to move the returns up a period.
"""

features_2007.shape

features_2007.groupby('securityID')['AdjClose'].pct_change() #This is get qtr over qtr returns, i.e. backward looking

features_2007['QoQ_Returns']=features_2007.groupby('securityID')['AdjClose'].pct_change()
features_2007['QoQ_Fwd_Returns']=features_2007.groupby('securityID')['QoQ_Returns'].shift(-1)

features_2007.loc[features_2007.ticker=='LEH',['qint','close','dividendFactor','AdjClose','QoQ_Returns','QoQ_Fwd_Returns']]

"""## Check Returns

* Check if returns are good.
* Are there outliers? Maybe trim them.
* If a stock is missing, the return should -99% or -100%.
* There is a survivorship bias right now, which is why the GICS analysis do not show LEH's GICS as a bad industry.

"""

#Do this better
features_2007['QoQ_Fwd_Returns']=features_2007['QoQ_Fwd_Returns'].clip(-0.5,2.0)



"""## Explore by GICS

* To Do:
  * This code here shows a simple average return group by GICS
  * There are a lot of small companies that are skewing the results
  * It might be better to weight by market cap.

* LEH's GICS is 40203020 which should show up as a bad industry
 * What are the other industries?

"""

allgicscode=features_2007.GICS.unique()

gics_returns=[]
for gc in allgicscode:
  if gc:
    thisdf=features_2007.loc[features_2007.GICS==gc,['qint','QoQ_Fwd_Returns']].groupby('qint').mean()
    thisdf.columns=[gc+'_FwdReturns']
    gics_returns.append(thisdf)
gics_returns_df=pd.concat(gics_returns,axis=1).iloc[:-1,]

gics_returns_df.index=gics_returns_df.index.map(str)

gics_cumprod=(1+gics_returns_df).cumprod()

# set the plotting backend to plotly
pd.options.plotting.backend = "plotly"

gics_cumprod.loc[:,gics_cumprod.loc['20084',:]<0.4].plot() #If graph goes down to 20084, since this is forward returns, it means the industry bottomed 3/2009.

s=gics_cumprod.loc['20084',:]<0.3
s[s]

s=gics_cumprod.loc['20084',:]<0.3
for g in s[s].index.map(lambda s:s.split('_')[0]):
  try:
    print(f'{g} {GICS(g,"20140228").level(4).name}')
  except:
    print(f'{g}')

"""## Something is wrong because these GICS industries are not the usual suspects!

* Let's plot prices of stocks that we knew went to or near $0 in 2008!

## Plotting price of Lehman Brothers

https://corporatefinanceinstitute.com/resources/capital-markets/lehman-brothers/
"""

# set the plotting backend to plotly
pd.options.plotting.backend = "plotly"
price_2007.dint=price_2007.dint.map(str)
price_2007.loc[price_2007.tic=='LEH',['Close','dint']].set_index('dint').plot()



"""## Plotting price of AIG

https://insight.kellogg.northwestern.edu/article/what-went-wrong-at-aig
"""

# set the plotting backend to plotly
pd.options.plotting.backend = "plotly"
price_2007.dint=price_2007.dint.map(str)
price_2007.loc[price_2007.tic=='AIG',['Close','dint']].set_index('dint').plot()

"""## Plotting price of FRE


https://en.wikipedia.org/wiki/Federal_takeover_of_Fannie_Mae_and_Freddie_Mac#:~:text=As%20of%202022%2C%20Fannie%20Mae,for%20an%20expected%20eventual%20exit.
"""

# set the plotting backend to plotly
pd.options.plotting.backend = "plotly"
price_2007.dint=price_2007.dint.map(str)
price_2007.loc[price_2007.tic=='FRE',['Close','dint']].set_index('dint').plot()

"""## Fixing survivorship bias

* Lehman was in Investment Banking & Brokerage but that sub-industry was not listed above as a poor performer.

"""

GICS(
  price_2007.loc[price_2007.tic=='LEH','GICS'].values[0]
).level(4).name

features_2007.loc[features_2007.ticker=='LEH',['qint','QoQ_Fwd_Returns']]

"""* The final quarter, there were no forward returns because Lehman stopped trading.
* We want to fill the final quarter with a bad return.
* You can set your own estimate. I put -95% here.


"""

s=features_2007.loc[:,'QoQ_Fwd_Returns'].isna()
features_2007.loc[s,'QoQ_Fwd_Returns']= -0.95

gics_returns=[]
for gc in allgicscode:
  if gc:
    thisdf=features_2007.loc[features_2007.GICS==gc,['qint','QoQ_Fwd_Returns']].groupby('qint').mean()
    thisdf.columns=[gc+'_FwdReturns']
    gics_returns.append(thisdf)
gics_returns_df=pd.concat(gics_returns,axis=1).iloc[:-1,]

gics_returns_df.index=gics_returns_df.index.map(str)

gics_cumprod=(1+gics_returns_df).cumprod()

# set the plotting backend to plotly
pd.options.plotting.backend = "plotly"

gics_cumprod.loc[:,gics_cumprod.loc['20084',:]<0.3].plot() #If graph goes down to 20084, since this is forward returns, it means the industry bottomed 3/2009.

s=gics_cumprod.loc['20084',:]<0.3
s[s]

s=gics_cumprod.loc['20084',:]<0.3
for g in s[s].index.map(lambda s:s.split('_')[0]):
  try:
    print(f'{g} {GICS(g,"20140228").level(4).name}')
  except:
    print(f'{g}')

"""Still not right as they industry returns are equal weight and that includes illiquid stocks that is not representative of the economy.

## Fixing micro cap bias (non-representative datapoints)

* Micro cap stocks have high variance in returns.
* When taking the simple average or mean in returns, we get a high variance because of the Micro cap stocks.
* One solution is to weigh by the inverse of volatility (or sqrt(variance)).
* Another solution is to drop these datapoints.
"""

#Drop all but the top 1600 names by the initial market cap in the 2007-2009 period.

features_2007.groupby('securityID')[['securityID','Market Cap']].head(1).set_index('securityID').sort_values('Market Cap').dropna().tail(1600)

liquidStocks=features_2007.groupby('securityID')[['securityID','Market Cap']].head(1).set_index('securityID').sort_values('Market Cap').dropna().tail(1600).index

features_2007=features_2007.loc[features_2007.securityID.isin(liquidStocks)]





gics_returns=[]
for gc in allgicscode:
  if gc:
    thisdf=features_2007.loc[features_2007.GICS==gc,['qint','QoQ_Fwd_Returns']].groupby('qint').mean()
    thisdf.columns=[gc+'_FwdReturns']
    gics_returns.append(thisdf)
gics_returns_df=pd.concat(gics_returns,axis=1).iloc[:-1,]

gics_returns_df.index=gics_returns_df.index.map(str)

gics_cumprod=(1+gics_returns_df).cumprod()

# set the plotting backend to plotly
pd.options.plotting.backend = "plotly"

gics_cumprod.loc[:,gics_cumprod.loc['20084',:]<0.4].plot() #If graph goes down to 20084, since this is forward returns, it means the industry bottomed 3/2009.

s=gics_cumprod.loc['20084',:]<0.4
for g in s[s].index.map(lambda s:s.split('_')[0]):
  try:
    print(f'{g} {GICS(g,"20140228").level(4).name}')
  except:
    print(f'{g}')

"""## Removing industries with a small number of stocks


"""

gics2counts=features_2007.GICS.value_counts().to_dict()

affectedGICS=pd.DataFrame()
s=gics_cumprod.loc['20084',:]<0.4
for g in s[s].index.map(lambda s:s.split('_')[0]):
  if gics2counts[g]<30:
    continue

  try:
    gicsName=GICS(g,"20140228").level(4).name
    print(f'{g} {gicsName} {gics2counts[g]}')
    affectedGICS.loc[g,'Sub-Industry']=gicsName
    affectedGICS.loc[g,'Count']=gics2counts[g]
  except:
    print(f'{g}')

"""These are the industries that really moved the market in 2008.
* Most affected by the GFC of 2008.

"""

affectedGICS.sort_index()

"""# Michael A

## To do -- Story telling: what happened in the 2008 GFC?

* Use the data above to tell a story.
"""

features_2007



# affected the most severely: Financial, Banks, Housing, Retail, Insurance, Office REITs
affectedGICS.loc[affectedGICS['Count']>150]

gics_cumprod['40102010_FwdReturns'].plot(title="Thrifts & Mortgage Finance")

"""In the early 2000s, there was a housing bubble in the United States, fueled by relaxed lending standards, low interest rates, and a high demand for mortgage-backed securities (MBS). Many individuals with low creditworthiness were able to obtain housing loans.

Subprime Mortgage Crisis: Banks and financial institutions began offering subprime mortgages to borrowers with poor credit histories or low incomes. These mortgages had adjustable interest rates, and borrowers often faced difficulties repaying their loans when rates increased.

Securitization and Derivatives: Mortgage loans were bundled together and sold as mortgage-backed securities (MBS) to investors. These securities were further divided and repackaged into complex financial instruments called collateralized debt obligations (CDOs). The complexity of these products made it difficult to assess their risks accurately.

"""

# Need GICS for: Homebuilding, Realty, Housing, etc
gics_cumprod['25201030_FwdReturns'].plot(title="Housing, Homebuilding, & Realty")

"""Burst of the Housing Bubble: In 2007, the housing bubble burst as housing prices began to decline, leading to a wave of foreclosures and a sharp decline in the value of MBS and CDOs. This exposed the weaknesses in the financial system and raised concerns about the solvency of major financial institutions."""

price_2007.loc[price_2007.tic=='LEH',['Close','dint']].set_index('dint').plot()

"""Banks and other financial institutions engaged in risky practices, such as investing heavily in MBS and CDOs, leveraging their positions with high levels of debt, and relying on short-term funding to finance long-term assets.

In September 2008, Lehman Brothers, a major investment bank heavily involved in the housing market, filed for bankruptcy. This event triggered a widespread loss of confidence in the financial markets and caused a panic among investors.

The crisis spread rapidly to other financial institutions around the world, leading to a liquidity freeze and a breakdown of trust in the interbank lending market. Stock markets plummeted, credit markets seized up, and many financial institutions faced insolvency.
"""

FICgics_cumprod = gics_cumprod[['40101015_FwdReturns','40101015_FwdReturns','40201020_FwdReturns']]
FICgics_cumprod

# 40203020, 40101015 40201020
#How do I graph '40101015_FwdReturns','40201020_FwdReturns' as well?
#FICgics_cumprod.set_index('qint').plot(title="Financial Institutions and Services")
gics_cumprod['40203020_FwdReturns'].plot(title="Financial Institutions and Services")

"""Governments and central banks intervened to stabilize the financial system. Measures included bailouts of troubled financial institutions, such as the Troubled Asset Relief Program (TARP) in the United States, and interest rate cuts to stimulate economic activity.

# 2 - Feature Engineering

## Match the Bloomberg Model
"""

# Many outliers
features_2022.std().sort_values().dropna().plot()

"""## Features to use:
```
fnames=['FCF/P', 'Income Tax', 'return252Days', 'Assets',
       'Working Capital', 'ROE', 'Accrual Ratio', 'Earnings Growth (4Y)',
       'Operating Income Before Depreciation', 'Market Cap', 'dividendFactor',
       'dollarVolume', 'Long Term Debt', 'EBIT/TEV', 'ROA',
       'Earnings Variability', 'shortInterestFloat', 'Depreciation',
       'IR252Days', 'Debt/Equity', 'CF/P', 'SG&A/Sales', 'Dividend', 'EBIT',
       'EBIT/P', 'Equity', 'S/P', 'Minority Interest', 'Operating Cash Flow',
       'Earnings', 'medianDollarVolume21Days',
       'volatility63Days', 'R&D', 'Sales Growth (3Y)', 'Operating Income',
       'Sales Growth (5Y)', 'Operating Expense', 'Interest Expense',
       'Earnings Growth (2Y)', 'B/P', 'Short Term Debt', 'Preferred Stock',
       'SG&A', 'Earnings Growth (3Y)', 'TEV', 'Operating Margin',
       'splitFactor', 'Cash', 'FCF', 'R&D/Sales', 'Sales Growth (1Y)',
       'Sales Variability', 'IR21Days', 'Sales Growth (2Y)', 'Sales',
       'Long Liabilities', 'Earnings Growth (5Y)', 'E/P',
       'Earnings Growth (1Y)', 'Sales Growth (4Y)',
       'Capital Expenditure', 'return21Days', 'Profit Margin']
```

PLUS use GICS with One-Hot encoding.

"""

fnames=['FCF/P', 'Income Tax', 'return252Days', 'Assets',
       'Working Capital', 'ROE', 'Accrual Ratio', 'Earnings Growth (4Y)',
       'Operating Income Before Depreciation', 'Market Cap', 'dividendFactor',
       'dollarVolume', 'Long Term Debt', 'EBIT/TEV', 'ROA',
       'Earnings Variability', 'shortInterestFloat', 'Depreciation',
       'IR252Days', 'Debt/Equity', 'CF/P', 'SG&A/Sales', 'Dividend', 'EBIT',
       'EBIT/P', 'Equity', 'S/P', 'Minority Interest', 'Operating Cash Flow',
       'Earnings', 'medianDollarVolume21Days',
       'volatility63Days', 'R&D', 'Sales Growth (3Y)', 'Operating Income',
       'Sales Growth (5Y)', 'Operating Expense', 'Interest Expense',
       'Earnings Growth (2Y)', 'B/P', 'Short Term Debt', 'Preferred Stock',
       'SG&A', 'Earnings Growth (3Y)', 'TEV', 'Operating Margin',
       'splitFactor', 'Cash', 'FCF', 'R&D/Sales', 'Sales Growth (1Y)',
       'Sales Variability', 'IR21Days', 'Sales Growth (2Y)', 'Sales',
       'Long Liabilities', 'Earnings Growth (5Y)', 'E/P',
       'Earnings Growth (1Y)', 'Sales Growth (4Y)',
       'Capital Expenditure', 'return21Days', 'Profit Margin']

#Trim the outliers
for c in fnames:
  features_2022[c]=features_2022[c].clip(features_2022[c].quantile(0.05),features_2022[c].quantile(0.95))
  features_2007[c]=features_2007[c].clip(features_2007[c].quantile(0.05),features_2007[c].quantile(0.95))

#Standardize the data
for c in fnames:
  for loop in range(5):
    features_2022[c]-=features_2022[c].mean()
    features_2022[c]/=features_2022[c].std()
    features_2022[c]=features_2022[c].fillna(0.0)

    features_2007[c]-=features_2007[c].mean()
    features_2007[c]/=features_2007[c].std()
    features_2007[c]=features_2007[c].fillna(0.0)

#code

"""## If you want to be ambitious, you can calculate some financial ratios here

You have already:
```       
'ROE', 'EBIT/TEV', 'E/P', 'S/P', 'B/P', 'CF/P', 'FCF/P', 'EBIT/P', 'SG&A/Sales'. Accrual Ratio, Profit Margin, ROA, ...
```

You can try calculating:
1. Market Leverage
2. Book Leverage

## Create Train-Test set

Todo: Repeat for every factor.

For example,
  * Leverage

## Choose several supervised continuous models

* Example here is Ridge Regression
"""

blgdf['Leverage']

"""We have several options for each ticker, so take the latest"""

features_2022_latest=features_2022.groupby('securityID').tail(1)

"""There are still duplicates in tickers, which will be a problem for indexing.

"""

features_2022_latest.ticker.value_counts().sort_values()

"""Research why tickers are duplicated

"""

features_2022_latest.loc[features_2022_latest.ticker=='COHR']

features_2022_latest.loc[features_2022_latest.ticker=='BHVN']

"""Resolve the duplicates

"""

features_2022_latest=features_2022_latest.loc[features_2022_latest.securityID!=31570101]
features_2022_latest=features_2022_latest.loc[features_2022_latest.securityID!=310940101]

"""Now reindex is possible

"""

features_2022_latest=features_2022_latest.set_index('ticker').reindex(blgdf.index).fillna(0)

features_2007_earliest=features_2007.groupby('securityID').head(1)
X_test=features_2007_earliest[fnames]

X=features_2022_latest[fnames]
y=blgdf['Leverage']

from sklearn.linear_model import RidgeCV

ridge_reg = RidgeCV()

# Fit the model using the training data
ridge_reg.fit(X, y)

# Make predictions on the test data
y_pred = ridge_reg.predict(X_test)

features_2007_earliest['Blg Leverage']=y_pred

#These are the leverage of all the companies that were in trouble in 2008.
features_2007_earliest.set_index('ticker')['Blg Leverage'].sort_values().loc[['LEH','AIG','WM','FRE','C','GM']]

y=blgdf['Size']
ridge_reg = RidgeCV()

# Fit the model using the training data
ridge_reg.fit(X, y)

# Make predictions on the test data
y_pred = ridge_reg.predict(X_test)

features_2007_earliest['Blg Size']=y_pred

y=blgdf['Value']
ridge_reg = RidgeCV()

# Fit the model using the training data
ridge_reg.fit(X, y)

# Make predictions on the test data
y_pred = ridge_reg.predict(X_test)

features_2007_earliest['Blg Value']=y_pred

y=blgdf['Profit']
ridge_reg = RidgeCV()

# Fit the model using the training data
ridge_reg.fit(X, y)

# Make predictions on the test data
y_pred = ridge_reg.predict(X_test)

features_2007_earliest['Blg Profit']=y_pred

"""#Kairui Hu


"""

def feateng(name):
  X=features_2022_latest[fnames]
  y=blgdf[name]
  ridge_reg = RidgeCV()

  # Fit the model using the training data
  ridge_reg.fit(X, y)

  # Make predictions on the test data
  y_pred = ridge_reg.predict(X_test)

  features_2007_earliest.loc[:, 'Blg '+ name] = y_pred

  return features_2007_earliest.set_index('ticker')['Blg ' + name].sort_values().loc[['LEH','AIG','WM','FRE','C','GM']]

blgdf.columns

# we can only do this with these, case Sector and Industry have characters and need to be onehot encoded
feateng("Activity")
feateng('Value')
feateng('Profit')
feateng('Size')
feateng("Leverage")
feateng("Ret")
feateng("Growth")
feateng("Momentum")
feateng("Variability")
feateng("Volatility")

#onehotencode industry and sector
onehotSector = pd.get_dummies(blgdf["Sector"])
onehotIndustry = pd.get_dummies(blgdf["Industry"])

#check sector is encoded
onehotSector

#check industry is encoded
onehotIndustry

#takes the onehot encoded df and then turns it into a 1d array, then does the stuff
import numpy as np

def onehotfeateng(df):
    X = features_2022_latest[fnames]
    y = np.argmax(df.values, axis=1)  # Convert one-hot encoded DataFrame to 1D array
    ridge_reg = RidgeCV()

    # Fit the model using the training data
    ridge_reg.fit(X, y)

    # Make predictions on the test data
    y_pred = ridge_reg.predict(X_test)

    column_name = 'Blg ' + df.columns[0]  # Create a column name based on the first column of the one-hot encoded DataFrame

    features_2007_earliest.loc[:, column_name] = y_pred

    return features_2007_earliest.set_index('ticker')[column_name].sort_values().loc[['LEH', 'AIG', 'WM', 'FRE', 'C', 'GM']]

#feature engineer Sector
# there is an error here as it adds alot of stuff to feature_2007_earliest which it is not suppose to i dont think.
onehotfeateng(onehotSector)

#feature engineer Sector
# same with this
onehotfeateng(onehotIndustry)

"""## To do - feature engineer more factors!

## To do - feature engineering, don't forget to add stock industries or sectors using One Hot Encoding on GICS!
"""



"""# 3 - Calculate Factor Returns using our new factor model

![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SxCzb8Djd8S2ME0yOrmpkg.png)


"""

factor_model_2007=features_2007_earliest[['ticker',"Blg Activity", 'Blg Value', 'Blg Profit', 'Blg Size', "Blg Leverage", "Blg Ret", "Blg Growth", "Blg Momentum", "Blg Variability", "Blg Volatility"]]
# the above only has the 4 things  you can copy and past the items below to get more of em execpt for the one hot encoded ones
# "Blg Activity", 'Blg Value', 'Blg Profit', 'Blg Size', "Blg Leverage", "Blg Ret", "Blg Growth", "Blg Momentum", "Blg Variability", "Blg Volatility"


factor_model_2007

#To do you can also use drop_duplicates
factor_model_2007=factor_model_2007.groupby('ticker').head(1).set_index('ticker')

"""Now calculate stock returns for 2007-2009"""

price_2007['Adj Close']=price_2007['Close']*price_2007['divFactor']
price_2007['Return']=price_2007.groupby('tic')['Adj Close'].pct_change()

price_2007.loc[price_2007.tic=='AAPL',['Adj Close','Return']]

2.278870/2.272182-1

2.260935/2.278870-1

"""## Consider using a weight least squares model

For example,
```
import numpy as np
import statsmodels.api as sm

# Generate some sample data
x = np.random.normal(size=100)
y = 0.5 * x + np.random.normal(size=100)

# Generate some sample weights
weights = np.random.random(size=100)

# Fit a weighted least squares regression model
model = sm.WLS(y, sm.add_constant(x), weights=1/weights)
results = model.fit()

# Print the results summary
print(results.summary())
```

## WLS To do:
* The weighted regression below has weights = 1.
* Create a better weighing scheme to reduce the variance of your model.
"""

import numpy as np
import statsmodels.api as sm

factor_returns={}
for dint in price_2007.dint.unique()[1:]:

  thisdf=price_2007.loc[price_2007.dint==dint]
  thisdf=thisdf.set_index('tic')
  commontics=list(set(factor_model_2007.index).intersection(thisdf.index))
  y=thisdf.loc[commontics,'Return'].clip(-0.5,0.5)
  X=factor_model_2007.loc[commontics]

  # Fit a weighted least squares regression model
  model = sm.WLS(y, sm.add_constant(X), weights= features_2007_earliest["Market Cap"].mean())
  results = model.fit()

  factor_returns[dint]=results.params.to_dict()


  #performs weighted least squares regression analysis
  #estimates the regression coefficients using the 'Return'
  #values as the dependent variable and the corresponding rows
  #from the 'factor_model_2007' dataset as the independent variables
  #The resulting regression coefficients are
  #stored in the factor_returns dictionary

"""# Kairui Hu"""

#apparently we just need to change weights ^ from 1 to something else
#he said you can do 1/vol or log(avg doller volume) or log(market cap)

# after some testing i found you can do
# features_2007_earliest["dollarVolume"].mean()
# features_2007_earliest["Market Cap"].mean()

# to get the dollarVolume, and Market cap average
# marketcap seems to work
# it does not like dollarVolume though



factor_returns_df=pd.DataFrame(factor_returns)
factor_returns_df.T.cumsum().plot()

#const - market return
#Aside from market, the factor that explained the crisis the most was Leverage
#Profitable stocks actually did fine.

import numpy as np
import statsmodels.api as sm

factor_returns={}
for dint in price_2007.dint.unique()[1:]:

  thisdf=price_2007.loc[price_2007.dint==dint]
  thisdf=thisdf.set_index('tic')
  commontics=list(set(factor_model_2007.index).intersection(thisdf.index))
  y=thisdf.loc[commontics,'Return'].clip(-0.5,0.5)
  X=factor_model_2007.loc[commontics]

  # Fit a weighted least squares regression model
  model = sm.WLS(y, X, weights= features_2007_earliest["Market Cap"].mean())
  results = model.fit()

  factor_returns[dint]=results.params.to_dict()

factor_returns_df=pd.DataFrame(factor_returns)
factor_returns_df.T.cumsum().plot()

#without a const term:
#Aside from market, the factor that explained the crisis the most was Leverage
#Profitable stocks actually did fine.



"""## Why did Leverage Meltdown happen?

DuPont Model

* CEOs in USA maximize ROE
* ROE = (net profit margin) x (asset turnover) x (financial leverage)
* ROE = (Net Income / Sales) x (Sales / Assets) x (Assets / Equity)
* ROE = (Net Income / Equity)

From 2002-2007, CEOs tried to maximize ROE.

* It was fashionable to do this by maximizing leverage.
* This was the era of financial engineering.
* Create a lot of insurance and synthetic products tied to low quality mortgages.
* Used fancy math to make AAA products out of low quality mortgages.





"""



"""# 4 - Explore by statistical factors from PCA.

You can follow what we did in this lecture and notebook

* Lecture: https://tinyurl.com/pca-stocks
* Notebook: https://colab.research.google.com/drive/1MRkD-0CaLmq5FByYUxBaLXDvt08AoaCV?usp=sharing

"""

# Create pivot table
pivot_table = pd.pivot_table(price_2007, values='Return1', index=['dint'], columns=['tic'])
X= pivot_table.fillna(0).clip(-0.4,0.4)

from sklearn.decomposition import PCA
pca = PCA(n_components := 8, random_state=42)
pca.fit(X)

#transformed_data is projecting the original data (dates,stocks) to a smaller dimension (dates,components)
transformed_data = pd.DataFrame(pca.transform(X), columns=[f'PCA{i+1}' for i in range(n_components)])
transformed_data.index=X.index

print('First PCA transforming first data point: ',pca.components_.T[:, 0].dot(X.iloc[0]))
print('First PCA transforming first data point: ',transformed_data.values[0][0])

print('Another transformation: ',pca.components_.T[:, 1].dot(X.iloc[5]))
print('Another transformation: ',transformed_data.values[5][1])

print('Another transformation: ',pca.components_.T[:, 7].dot(X.iloc[15]))
print('Another transformation: ',transformed_data.values[15][7])

import matplotlib.pyplot as plt

# plot the significance of each principal component
plt.plot(range(1, pca.n_components_+1), pca.explained_variance_ratio_, 'ro-', linewidth=2)
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Proportion of Variance Explained')
plt.show()



pca.components_.shape #(8 pcas, 1000 stocks)

# extract the loadings for the first principal component
loadings = pd.DataFrame(pca.components_.T[:, :], index=X.columns, columns=[f'PCA{i+1}' for i in range(n_components)])
for c in loadings.columns:
  loadings[c]-=loadings[c].mean()
  loadings[c]/=loadings[c].std()
loadings

loadings.loc[['LEH','AIG','C','FRE','WM','GM']]

loadings.loc[['LEH','AIG','C','FRE','WM','GM']].mean()

loadings.loc[loadings.PCA5>2.5]

sometics=loadings.loc[loadings.PCA5>2.5].index
result_df=pd.DataFrame(sometics.map(price_2007.set_index('tic')['BusinessDescription'].to_dict()))
result_df.index=sometics
result_df.rename(columns={'tic':'desc'})

"""# Michael A

* Which stocks were clustered together using PCA, and why?
* What there a cluster that was most indicative of the Global Financial Crisis?

  * PCA5 has a lot of the poor performers

The main types of stocks that were clustered together using PCA were stocks in industries that were affected similarly by the GFC. For example, looking at PCA5 shows us a group of stocks from Financial, Mortgage and home based companies.

PCA5 has a lot of bad performing stocks from the financial sector which is why they were all clustered together. They had similar data points.

## Experiment

1. Partition your price dataset into 2-3 parts.
2. Was PCA5 or Leverage an apparent factor at the start or middle of the crisis?

* Our goal right now is to find the next SIVB or the next shoe to drop...
  * SIVB/FRC went to $0
  * What is next.
"""

#Say in 10/2007 (peak), 5/2008 (middle of the crisis), would you be able to identify bad stocks using PCA/Fundamental factors?
price_2007

#Identify bad stocks using PCA / Fundamental factors

#Split price dataset into 2 or 3 parts
price_2007_1 = price_2007.iloc[:int(len(price_2007)/2),:]
price_2007_2 = price_2007.iloc[int(len(price_2007)/2):,:]

price_2007_1

price_2007_2

sometics = loadings.loc[loadings.PCA1>2.5].index
result_df=pd.DataFrame(sometics.map(price_2007.set_index('tic')['BusinessDescription'].to_dict()))
result_df.index=sometics
result_df.rename(columns={'tic':'desc'})

# Financial and housing corps, again!

"""CEOs were maximizing leverage between 2002-2007. The leverage meltdown  shows that leverage was an apparent factor throughout the beginning and middle of crisis.




"""

# first identify how to view one tiny piece of PCA5
early_aig = price_2007_1.loc[price_2007_1['tic']=='AIG']
early_aig.plot(x='dint',y='Return1')

# now do it for all

# interchangeable stock to test factor
stock_to_view = 'FRE'

xpca5 = loadings[loadings['PCA5']>2.5].index
early_xpca5 = price_2007_1[price_2007_1['tic'].isin(xpca5)]
early_xpca5[early_xpca5['tic']==stock_to_view].plot(x='dint',y='Return1')

xpca5 = loadings[loadings['PCA5']>2.5].index
late_xpca5 = price_2007_2[price_2007_2['tic'].isin(xpca5)]
late_xpca5[late_xpca5['tic']==stock_to_view].plot(x='dint',y='Return1')

"""According to PCA analysis, PCA5 had erratic changes in features, such as returns, during the middle of the crisis. This may prove that there was panic amongst financial stocks

# 5 - Compare and Constrast to 2023

* What are the factor returns from 2022-2023?
* What stocks are heavily clustered together from 2022-2023?
* Does the situation resemble 2008?

#Michael A
"""

factor_model_2007.plot(kind='hist')

# factor returns for 2022-2023: 'Size',	'Ret',	'Yield',	'Activity',	'Growth',	'Leverage',	'Momentum',	'Variability',	'Volatility',	'Value'	,'Profit'
blgdf[['Size',	'Ret',	'Yield',	'Activity',	'Growth',	'Leverage',	'Momentum',	'Variability',	'Volatility',	'Value'	,'Profit']].plot(kind='hist')

"""Stocks with near 0 returns and yields of -0.8 seem to be clustered together from 2022-2023.

Yes, there is some resemblance between 2007 and 2022. However, 2022-2023 shows more evidence of stocks with positive factors accross the board, however there is a significant amount of negative values just like in 2007-2008.  The economy in 2022-2023 is not as drastically negative as it was in 2007-2008, but there are significant similarities to drops in factors between both economic timeframes.
"""



"""# 6 - Finance only: How would you navigate the Global Financial Crisis?

Portfolio Optimization:
1. Covariance matrix based on the first half of the data, say 2006-2007.
2. An alpha model assuming you had some foresight. For example, you can choose to short leverage and long profitability as per above example.
3. Optimize a market neutral model such as the sum of the positive weights = 1, and the sum of the negative weights = -1, with the highest IR (i.e. ratio of portfolio alpha/portfolio risk)
4. Apply reasonable constraints such as maximum weight to any given stock or industry. (i.e. Not 100% short LEH and 100% long AAPL)
5. Compare optimizer, monte carlo and genetic algorithm outputs and computing performances.


## See Minimum Variance (Monte Carlo, Optimizer, Genetic Algo)
  * Python Notebook: https://colab.research.google.com/drive/19_8ZmK7tf0vb7dZuzzWm_sCvJG7qgn3b?usp=sharing
  * Lecture Video: https://tinyurl.com/minvarport

"""

X= pivot_table.fillna(0).clip(-0.4,0.4) #To do: determine the best trimming
commontics=list(set(factor_model_2007.index).intersection(X.columns))

firsthalf=X.loc[:,commontics].head(500)
secondhalf=X.loc[:,commontics].tail(-500)

# For example, say we have the foresight to know that leverage is the big loser in 2008.

alpha = -1 * factor_model_2007['Blg Leverage'] #To do: determine your alpha model
alpha = alpha.loc[commontics]

sampleCov=firsthalf.cov()
sampleCov

"""For Monte Carlo, we need a large number of samples for s Monte Carlo simulation...

"""

n_samples = 50000
riskfree = 0.01
monteWeights = np.random.dirichlet(np.ones(len(alpha)), n_samples)
monteWeights.shape #Each row represents a random sample of a 91 dimensional dirichlet distribution with alphas all set to 1 (uniformative prior)

exprets = monteWeights.dot(alpha)

exp_variances=(monteWeights.T * (sampleCov @ monteWeights.T)).sum(axis=0)
exp_variances.shape

stds = np.sqrt(exp_variances)

sharpes = (exprets-riskfree) / stds

print("Sample portfolio min vol:", stds.min())
print("Sample portfolio max sharpe:", sharpes.max())

fig, ax = plt.subplots()
ax.scatter(stds, exprets, marker=".", c=sharpes, cmap="viridis_r")
ax.set_title("Efficient Frontier with random portfolios")

X=np.arange(stds.min(),stds.max(),0.01)
y=riskfree + sharpes.max()*X
ax.plot(X,y,color='red')

plt.ylabel('Portfolio Returns')
plt.xlabel('Portfolio Risks')

plt.tight_layout()
plt.show()

"""## Long only example

* Decide which portfolio you would trade on 2007-12-28 given your view on covariance (naive view right now, it is just the sample covariance), and your view on alpha.
* Repeat for short-only
* Repeat for market neutral - say 100% long vs 100% short.
* Plot the returns after your portfolio formation date on 2007-12-28
* Determine if your strategy worked, and refine your strategy.

"""

secondhalf

"""## Now give a recommendation on the best market neutral portfolio in 2023.

* Balance alpha and volatility
* Where alpha is your views on which factors will outperform and underperform
* Where volatility is based on your covariance matrix.

## You can also use GICS for above.
"""





"""# 7 - Finance only: Detect the GFC?

Markov switching dynamic regression models:
1. Apply a Markov switching dynamic regression models on 2007-2009 to see if there a several regimes that might be triggered as the crisis unfolded.
2. What factors were affected by the adverse regime?
3. When did the crisis end according to the regime model?


## See Markov switching dynamic regression models (Are we in a Recession?)
  * Reference: https://www.statsmodels.org/dev/examples/notebooks/generated/markov_regression.html
  * Python Notebook: https://colab.research.google.com/drive/1yTNDhBgqSiyycF-MTVsVaAfQuTnDHy4Z?usp=sharing
  * Lecture Video: https://tinyurl.com/markovswitch

"""



